{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246b01f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "All datasets loaded and merged successfully.\n",
      "Full data spans from 2014-03-01 00:00:00 to 2024-10-31 23:00:00\n",
      "\n",
      "Train set shape: (76007, 4)\n",
      "Validation set shape: (8760, 4)\n",
      "Test set shape: (8777, 4)\n",
      "\n",
      "Starting LSTM model training for 20-hour forecast...\n",
      "Epoch [1/200], Validation Loss: 0.001111\n",
      "Epoch [2/200], Validation Loss: 0.000974\n",
      "Epoch [3/200], Validation Loss: 0.000924\n",
      "Epoch [4/200], Validation Loss: 0.000807\n",
      "Epoch [5/200], Validation Loss: 0.000770\n",
      "Epoch [6/200], Validation Loss: 0.000762\n",
      "Epoch [7/200], Validation Loss: 0.000843\n",
      "Epoch [8/200], Validation Loss: 0.000741\n",
      "Epoch [9/200], Validation Loss: 0.000709\n",
      "Epoch [10/200], Validation Loss: 0.000760\n",
      "Epoch [11/200], Validation Loss: 0.000686\n",
      "Epoch [12/200], Validation Loss: 0.000763\n",
      "Epoch [13/200], Validation Loss: 0.000692\n",
      "Epoch [14/200], Validation Loss: 0.000716\n",
      "Epoch [15/200], Validation Loss: 0.000687\n",
      "Epoch [16/200], Validation Loss: 0.000761\n",
      "Epoch [17/200], Validation Loss: 0.000687\n",
      "Epoch [18/200], Validation Loss: 0.000669\n",
      "Epoch [19/200], Validation Loss: 0.000728\n",
      "Epoch [20/200], Validation Loss: 0.000689\n",
      "Epoch [21/200], Validation Loss: 0.000715\n",
      "Epoch [22/200], Validation Loss: 0.000695\n",
      "Epoch [23/200], Validation Loss: 0.000681\n",
      "Epoch [24/200], Validation Loss: 0.000698\n",
      "Epoch [25/200], Validation Loss: 0.000773\n",
      "Epoch [26/200], Validation Loss: 0.000698\n",
      "Epoch [27/200], Validation Loss: 0.000692\n",
      "Epoch [28/200], Validation Loss: 0.000654\n",
      "Epoch [29/200], Validation Loss: 0.000749\n",
      "Epoch [30/200], Validation Loss: 0.000705\n",
      "Epoch [31/200], Validation Loss: 0.000770\n",
      "Epoch [32/200], Validation Loss: 0.000680\n",
      "Epoch [33/200], Validation Loss: 0.000680\n",
      "Epoch [34/200], Validation Loss: 0.000665\n",
      "Epoch [35/200], Validation Loss: 0.000672\n",
      "Epoch [36/200], Validation Loss: 0.000750\n",
      "Epoch [37/200], Validation Loss: 0.000702\n",
      "Epoch [38/200], Validation Loss: 0.000669\n",
      "Early stopping triggered.\n",
      "\n",
      "--- LSTM Model Performance Summary (20-Hour Forecast) ---\n",
      "1. Custom Accuracy for Monsoon Season (June-Oct): 74.92%\n",
      "2. Custom Accuracy for Full Year: 88.76%\n",
      "3. Custom Accuracy for Dry Season (Nov-May): 98.81%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- Basic Setup ---\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "INPUT_STEPS = 48\n",
    "FORECAST_HOUR = 20\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Data Loading ---\n",
    "def load_and_merge_data():\n",
    "    \"\"\"Loads all data files, merges them, and sets a datetime index.\"\"\"\n",
    "    try:\n",
    "        old_chatia_df = pd.read_excel('Chatia_train.xlsx')\n",
    "        old_rewaghat_df = pd.read_excel('Rewaghat_train.xlsx')\n",
    "        new_chatia_df = pd.read_excel('Chatia_test.xlsx')\n",
    "        new_rewaghat_df = pd.read_excel('Rewaghat_test.xlsx')\n",
    "        rainfall_df = pd.read_excel('rainfall_data.xlsx')\n",
    "        dumariaghat_df = pd.read_excel('Dumariaghat_data.xlsx')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Please ensure all data files are in the script's directory.\")\n",
    "        exit()\n",
    "\n",
    "    full_chatia_df = pd.concat([old_chatia_df, new_chatia_df]).drop_duplicates(subset=['Date'])\n",
    "    full_rewaghat_df = pd.concat([old_rewaghat_df, new_rewaghat_df]).drop_duplicates(subset=['Date'])\n",
    "\n",
    "    for df_ in [full_chatia_df, full_rewaghat_df, rainfall_df, dumariaghat_df]:\n",
    "        df_['Date'] = pd.to_datetime(df_['Date'], format='%d-%m-%Y %H:%M')\n",
    "\n",
    "    dumariaghat_df = dumariaghat_df.drop_duplicates(subset=['Date'])\n",
    "\n",
    "    df_merged = pd.merge(full_chatia_df, full_rewaghat_df, on='Date', how='inner')\n",
    "    df_merged = pd.merge(df_merged, dumariaghat_df[['Date', 'Dumariaghat']], on='Date', how='inner')\n",
    "    df_merged = pd.merge(df_merged, rainfall_df, on='Date', how='inner')\n",
    "\n",
    "    df_merged.set_index('Date', inplace=True)\n",
    "    df_merged.sort_index(inplace=True)\n",
    "    df_merged.rename(columns={'Chatia': 'Chatia', 'Rewaghat': 'Rewaghat', 'Dumariaghat': 'Dumariaghat', 'Rainfall': 'Rainfall'}, inplace=True)\n",
    "    \n",
    "    print(\"All datasets loaded and merged successfully.\")\n",
    "    print(f\"Full data spans from {df_merged.index.min()} to {df_merged.index.max()}\")\n",
    "    return df_merged[['Chatia', 'Rewaghat', 'Dumariaghat', 'Rainfall']]\n",
    "\n",
    "# --- LSTM Data Preparation ---\n",
    "def create_sequences_for_single_step(data, input_steps, forecast_hour):\n",
    "    \"\"\"Creates input sequences (X) and a single target value (y) for the LSTM.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_steps - forecast_hour + 1):\n",
    "        X.append(data[i:(i + input_steps)])\n",
    "        # Target 'y' is the single value at the 20th hour ahead (Rewaghat is column index 1)\n",
    "        y.append(data[i + input_steps + forecast_hour - 1, 1])\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "\n",
    "# --- LSTM Model Definition ---\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Sequence-to-Vector LSTM model that predicts a single future time step.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, 1) # Output size is 1 for single-step prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(NUM_LAYERS, x.size(0), HIDDEN_SIZE).to(x.device)\n",
    "        c0 = torch.zeros(NUM_LAYERS, x.size(0), HIDDEN_SIZE).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :]) # Use only the last time step's output\n",
    "        return out\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_network(model, train_loader, val_loader, criterion, optimizer):\n",
    "    \"\"\"Trains the LSTM model with early stopping.\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                outputs = model(X_batch)\n",
    "                val_loss += criterion(outputs, y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Validation Loss: {val_loss:.6f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save(best_model_state, 'best_lstm_comparison_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "            \n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# --- Evaluation Function (Mirrors XGBoost output) ---\n",
    "def evaluate_seasonal_accuracy(model, test_df, test_dates, scaler):\n",
    "    \"\"\"Evaluates the model and prints accuracy for different seasons.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Prepare test data and make predictions\n",
    "    scaled_test_data = scaler.transform(test_df)\n",
    "    X_test_seq, y_test_seq = create_sequences_for_single_step(scaled_test_data, INPUT_STEPS, FORECAST_HOUR)\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test_seq), torch.FloatTensor(y_test_seq))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            outputs = model(X_batch)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actuals = y_test_seq.flatten()\n",
    "\n",
    "    # 2. Inverse transform predictions and actuals\n",
    "    dummy_preds = np.zeros((len(predictions), 4))\n",
    "    dummy_preds[:, 1] = predictions\n",
    "    inv_predictions = scaler.inverse_transform(dummy_preds)[:, 1]\n",
    "\n",
    "    dummy_actuals = np.zeros((len(actuals), 4))\n",
    "    dummy_actuals[:, 1] = actuals\n",
    "    inv_actuals = scaler.inverse_transform(dummy_actuals)[:, 1]\n",
    "\n",
    "    # 3. Create results DataFrame with correct dates\n",
    "    result_dates = test_dates[INPUT_STEPS + FORECAST_HOUR - 1:]\n",
    "    results_df = pd.DataFrame({\n",
    "        'Date': result_dates,\n",
    "        'Actual': inv_actuals,\n",
    "        'Predicted': inv_predictions\n",
    "    }).set_index('Date')\n",
    "    \n",
    "    # 4. Filter by season (same as XGBoost notebook)\n",
    "    monsoon_months = [6, 7, 8, 9, 10]\n",
    "    monsoon_df = results_df[results_df.index.month.isin(monsoon_months)]\n",
    "    dry_df = results_df[~results_df.index.month.isin(monsoon_months)]\n",
    "    \n",
    "    def calculate_custom_accuracy(df):\n",
    "        if df.empty: return 0.0\n",
    "        errors = df['Predicted'] - df['Actual']\n",
    "        return np.mean((errors >= -0.15) & (errors <= 0.15)) * 100\n",
    "        \n",
    "    # 5. Calculate accuracies\n",
    "    accuracy_monsoon = calculate_custom_accuracy(monsoon_df)\n",
    "    accuracy_dry = calculate_custom_accuracy(dry_df)\n",
    "    accuracy_full = calculate_custom_accuracy(results_df)\n",
    "\n",
    "    # 6. Print in the desired format\n",
    "    print(\"\\n--- LSTM Model Performance Summary (20-Hour Forecast) ---\")\n",
    "    print(f\"1. Custom Accuracy for Monsoon Season (June-Oct): {accuracy_monsoon:.2f}%\")\n",
    "    print(f\"2. Custom Accuracy for Full Year: {accuracy_full:.2f}%\")\n",
    "    print(f\"3. Custom Accuracy for Dry Season (Nov-May): {accuracy_dry:.2f}%\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and merge all data\n",
    "    df = load_and_merge_data()\n",
    "    \n",
    "    # ** NEW: Time-based split identical to XGBoost notebook **\n",
    "    train_df = df.loc[df.index < (df.index.max() - pd.DateOffset(years=2))]\n",
    "    val_df = df.loc[(df.index >= (df.index.max() - pd.DateOffset(years=2))) & \n",
    "                    (df.index < (df.index.max() - pd.DateOffset(years=1)))]\n",
    "    test_df = df.loc[df.index >= (df.index.max() - pd.DateOffset(years=1))]\n",
    "    \n",
    "    print(f\"\\nTrain set shape: {train_df.shape}\")\n",
    "    print(f\"Validation set shape: {val_df.shape}\")\n",
    "    print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_train_data = scaler.fit_transform(train_df)\n",
    "    scaled_val_data = scaler.transform(val_df)\n",
    "    \n",
    "    # Create sequences for the LSTM\n",
    "    X_train, y_train = create_sequences_for_single_step(scaled_train_data, INPUT_STEPS, FORECAST_HOUR)\n",
    "    X_val, y_val = create_sequences_for_single_step(scaled_val_data, INPUT_STEPS, FORECAST_HOUR)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = LSTMModel(input_size=4, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting LSTM model training for 20-hour forecast...\")\n",
    "    trained_model = train_network(model, train_loader, val_loader, criterion, optimizer)\n",
    "    \n",
    "    # Evaluate and print results in the desired format\n",
    "    evaluate_seasonal_accuracy(trained_model, test_df, test_df.index, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b5675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
